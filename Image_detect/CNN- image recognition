import numpy as np
import pandas as pd
from keras.layers import Input, Dense, Activation, ZeroPadding2D, Flatten, Conv2D
from keras.layers import MaxPooling2D
from keras.models import Model
from keras.preprocessing import image
from keras.models import load_model
from keras import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from keras.applications.imagenet_utils import preprocess_input
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot
from keras.utils import plot_model
from PIL import Image
import keras.backend as K
import tensorflow as tf
import keras
from keras.wrappers.scikit_learn import KerasClassifier
import csv
K.set_image_data_format('channels_last')
from matplotlib.pyplot import imshow
import os


img_width = 300
img_height = 600
VECTOR_SIZE = 128
features_SIZE = 7
BATCH_SIZE = 32
EPOCHS = 1000
TRAIN_EXAMPLES = 100
TEST_EXAMPLES = 100
LR = 0.0001
def mean_pred(y_true, y_pred):
    return K.mean(y_pred)


# Crop and rotate image, return 12 images
def getCropImgs(img, needRotations=False):
    # img = img.convert('L')
    z = np.asarray(img, dtype=np.int8)
    c = []
    for i in range(3):
        for j in range(4):
            crop = z[512 * i:512 * (i + 1), 512 * j:512 * (j + 1), :]

            c.append(crop)
            if needRotations:
                c.append(np.rot90(np.rot90(crop)))

    # os.system('cls')
    # print("Crop imgs", c[2].shape)

    return c

# Get the softmax from folder name
def getAsSoftmax(fname):
    if (fname == 'b'):
        return [1, 0, 0, 0]
    elif (fname == 'is'):
        return [0, 1, 0, 0]
    elif (fname == 'iv'):
        return [0, 0, 1, 0]
    else:
        return [0, 0, 0, 1]


# Return all images as numpy array, labels
def get_imgs_frm_folder(path):
    # x = np.empty(shape=[19200,512,512,3],dtype=np.int8)
    # y = np.empty(shape=[400],dtype=np.int8)

    x_image = []
    y_image = []

    cnt = 0
    for filename in os.listdir(path):
        image_path = os.path.join(path, filename)
        img = Image.open(image_path)
        #print(img.size)
        img = img.convert('RGB')
        img = np.asarray(img, np.float16) #(300, 600, 3)
        #print(img.shape)
        '''
        if len(img.shape) == 2:
            img = np.concatenate((img, img, img), axis=-1)
            #img = np.reshape(img,(min_width,min_height,1))
        print
        '''
        x_image.append(np.divide(img, 255.))
        y_image.append(filename)
        cnt += 1
        '''
        crpImgs = getCropImgs(img)
        cnt += 1
        if cnt % 10 == 0:
            print(str(cnt) + " Images loaded")
        for im in crpImgs:
            x.append(np.divide(np.asarray(im, np.float16), 255.))
            # Image.fromarray(np.divide(np.asarray(im, np.float16), 255.), 'RGB').show()
            y.append( (foldname))
            # print(getAsSoftmax(foldname))
        '''
    print("Images cropped")
    print("Loading as array")
    print("count: ",cnt)
    return x_image, y_image, cnt

def get_one_hot(data, index):
    for i in index:
        tempdata = data[:,i].reshape(-1, 1)
        enc = OneHotEncoder()
        enc.fit(tempdata)
        tempdata = enc.transform(tempdata).toarray()
        data = np.concatenate((data,tempdata),axis=1)
    for i in reversed(index):
        data = np.delete(data,i,axis=1)
    return data

# Load the dataset
def load_dataset(image_path,feats_path):
    print("Loading images..")

    train_set_x_orig, train_set_y_orig, cnt = get_imgs_frm_folder(image_path)

    feats_train = pd.read_csv(feats_path)
    #feats_train = feats_train.join(pd.get_dummies(feats_train.HER2))
    feats_train = feats_train.fillna(method='backfill') #handle the NAN
    print(feats_train.head())
    X_img = []
    X_fea = []
    Y = []

    for i in range(len(train_set_y_orig)):
        sample = []
        
        seg_id = train_set_y_orig[i].split('_')[1].split('.')[0]
        if len(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])]) == 0:
            continue
        X_img.append(train_set_x_orig[i])
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['num_collisions'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['num_cases_injury_other_visible'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['num_cases_complaint_of_pain'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['num_cases_severe'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['num_cases_fatal'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['cyc_ntwrk_yn'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['speed_limit'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['pk_metered_cnt'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['pk_on_st_cnt'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['oneway_yn'] )
        sample.append( float(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['daily_ride_qrt'] ))
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['facility_type'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['surface_type'] )
        sample.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['sharrow'] )

        Y.append(feats_train[feats_train['f_node_cnn_intrsctn_fkey'].isin([seg_id])].iloc[0]['y'])
        X_fea.append(sample)
    
    X_img = np.array(X_img,dtype = np.float)
    Y = np.array(Y,dtype = np.float).reshape((-1,1))
    X_fea = np.array(X_fea)
    #X_fea[:,10] = X_fea[:,10]/X_fea[:,10].max()
    X_fea = get_one_hot(X_fea,index = [5,9,11,12,13])
    X_fea = np.array(X_fea,dtype = np.float)
    X_fea[:,8] = X_fea[:,8]/X_fea[:,8].max()
    
    print(X_img.shape)
    print(X_fea.shape)
    print(Y.shape)
    x_img_train, x_img_test = train_test_split(X_img, train_size=0.70, random_state=33)
    y_train, y_test = train_test_split(Y,train_size=0.70, random_state=33)
    x_fea_train, x_fea_test = train_test_split(X_fea, train_size=0.70, random_state=33)
    TRAIN_EXAMPLES = len(x_fea_train)
    TEST_EXAMPLES = len(x_fea_test)

    
    print("Data load complete")

    return x_img_train, x_img_test, y_train, y_test, x_fea_train, x_fea_test



def weight_variable(shape):
    initial = tf.truncated_normal(shape, mean=0.0, stddev=0.01)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape = shape)
    return tf.Variable(initial)

def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding = 'SAME')

def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1],
                          strides = [1, 2, 2, 1], padding = 'SAME')


def train(batch_size, epochs,x_img_train, x_img_test, y_train, y_test, x_fea_train, x_fea_test):#BATCH_SIZE, EPOCHS,x_img_train, x_img_test, y_train, y_test, x_fea_train, x_fea_test
    x_img = tf.placeholder(tf.float32,[None, img_width,img_height,3])
    x_fea = tf.placeholder(tf.float32,[None, x_fea_train.shape[1]])
    keep_prob = tf.placeholder("float")
    y_ = tf.placeholder("float", [None, 1])
    
    x_image = tf.reshape(x_img, [-1, img_width, img_height, 3])  
    W_conv1 = weight_variable([5, 5, 3, 8])
    b_conv1 = bias_variable([8])
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)
    
    W_conv2 = weight_variable([5, 5, 8, 16])
    b_conv2 = bias_variable([16])
    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    h_pool2 = max_pool_2x2(h_conv2)

    W_conv3 = weight_variable([5, 5, 16, 32])
    b_conv3 = bias_variable([32])
    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)
    h_pool3 = max_pool_2x2(h_conv3)
    print("h_pool3:",h_pool3.shape)

    W_fc1 = weight_variable([38 * 75 * 32, 2048])
    b_fc1 = bias_variable([2048])

    h_pool3_flat = tf.reshape(h_pool3, [-1, 38 * 75 * 32])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)
    print("h_fc1:",h_fc1.shape)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

    W_fc2 = weight_variable([2048, VECTOR_SIZE])
    b_fc2 = bias_variable([VECTOR_SIZE])
    #output of CNN
    print("h_fc1_drop:",h_fc1_drop.shape)
    print("W_fc2:",W_fc2.shape)
    cnn_out =  tf.matmul(h_fc1_drop, W_fc2) + b_fc2
    print("cnn_out:",cnn_out.shape)
    cnn_out = tf.concat([cnn_out,x_fea],axis = 1)
    print("cnn_out:",cnn_out.shape)
    
    W_fc2 = weight_variable([VECTOR_SIZE+x_fea_train.shape[1], 1024])
    b_fc2 = bias_variable([1024])
    h_fc2 = tf.nn.relu(tf.matmul(cnn_out, W_fc2) + b_fc2)
    h_fc2 = tf.nn.dropout(h_fc2, keep_prob)

    W_fc3 = weight_variable([1024, 512])
    b_fc3 = bias_variable([512])
    h_fc3 = tf.nn.relu(tf.matmul(h_fc2, W_fc3) + b_fc3)
    h_fc3 = tf.nn.dropout(h_fc3, keep_prob)

    W_fc4 = weight_variable([512, 256])
    b_fc4 = bias_variable([256])
    h_fc4 = tf.nn.relu(tf.matmul(h_fc3, W_fc4) + b_fc4)
    h_fc4 = tf.nn.dropout(h_fc4, keep_prob)

    W_fc5 = weight_variable([256, 128])
    b_fc5 = bias_variable([128])
    h_fc5 = tf.nn.relu(tf.matmul(h_fc4, W_fc5) + b_fc5)
    h_fc5 = tf.nn.dropout(h_fc5, keep_prob)


    W_fc_out = weight_variable([128, 1])
    b_fc_out = bias_variable([1])
    y_pred = tf.nn.relu(tf.matmul(h_fc5, W_fc_out) + b_fc_out)

    print("y_pred:",y_pred.shape)

    loss = tf.reduce_mean(tf.square(y_-y_pred))

    train_step = tf.train.AdamOptimizer(LR).minimize(loss) 


    x_img_train = np.array(x_img_train)
    print(x_img_train.shape)
    x_fea_train = np.array(x_fea_train)
    x_fea_train = np.reshape(x_fea_train,(x_fea_train.shape[0],x_fea_train.shape[1]))

    x_fea_test = np.array(x_fea_test)
    x_fea_test = np.reshape(x_fea_test,(x_fea_test.shape[0],x_fea_test.shape[1]))



    sess = tf.Session() 
    #sess.run(tf.initialize_all_variables())
    sess.run(tf.global_variables_initializer()) 
    results_train = []
    results_test = []
    for i in range(epochs): 
        print("epoch:",i)
        test_losses=[]
        train_losses = []

        for j in range(TRAIN_EXAMPLES//BATCH_SIZE):

            _,train_loss = sess.run(fetches = (train_step,loss),
                                           feed_dict = {x_img:x_img_train[j*BATCH_SIZE:(j+1)*BATCH_SIZE], x_fea:x_fea_train[j*BATCH_SIZE:(j+1)*BATCH_SIZE], y_:y_train[j*BATCH_SIZE:(j+1)*BATCH_SIZE], keep_prob:0.5})
            train_losses.append(train_loss)
        print("step %d, train_loss %g" %(i, sum(train_losses) / len(train_losses)))
        results_train.append(sum(train_losses) / len(train_losses))
        for j in range(TEST_EXAMPLES//BATCH_SIZE):

            test_loss = sess.run(fetches = (loss),  feed_dict = {x_img:x_img_test[j*BATCH_SIZE:(j+1)*BATCH_SIZE],x_fea:x_fea_test[j*BATCH_SIZE:(j+1)*BATCH_SIZE], y_:y_test[j*BATCH_SIZE:(j+1)*BATCH_SIZE],
                       keep_prob:1.0}) #神经元输出保持不变的概率 keep_prob 为0.5
            test_losses.append(test_loss)

        print("step %d, test_loss %g" %(i, sum(test_losses) / len(test_losses)))
        results_test.append(sum(test_losses) / len(test_losses))
    
    '''
    ###evaluation
    X_img,X_fea,ID = load_test_dataset(image_test_path,feats_test_path)
    X_img = np.array(X_img)
    print(X_img.shape)
    X_fea = np.array(X_fea)
    X_fea = np.reshape(X_fea,(X_fea.shape[0],X_fea.shape[1]))

    id_pred = {}
    result = []
    for i in range(len(ID)):
        if ID[i] not in id_pred.keys():
            id_pred.update({ID[i]: [0,0,0,0]})

        pred = sess.run(fetches = (y_pred), feed_dict = {x_fea:np.reshape(X_fea[i],(1,X_fea[i].shape[0])), keep_prob:1.0})
        print(i,pred)
        id_pred[ID[i]][pred[0]] += 1
    out = open('prediction.csv','w', newline='')
    csv_write = csv.writer(out,dialect='excel')
    print(id_pred)
    for key in id_pred:
        csv_write.writerow([key,id_pred[key].index(max(id_pred[key])) + 1])
    print ("write over")
    import matplotlib.pyplot as plt

    plt.plot(results_train, label='train mse')
    plt.plot(results_test, label='test mse')
    plt.title('DNN')
    plt.legend()
    plt.show()


        preds = model.evaluate(X_test, Y_test_orig, batch_size=1, verbose=1, sample_weight=None)
        print(preds)

        print()
        print("Loss = " + str(preds[0]))
        print("Test Accuracy = " + str(preds[1]) + "\n\n\n\n\n")
        ch = input("Do you wish to continue training? (y/n) ")
        if ch == 'y':
            epochs = int(input("How many epochs this time? : "))
            continue
        else:
            break
    '''

    return 


def predict(img, savedModelPath, showImg=True):
    model = load_model(savedModelPath)
    # if showImg:
    # Image.fromarray(np.array(img, np.float16), 'RGB').show()

    x = img
    if showImg:
        Image.fromarray(np.array(img, np.float16), 'RGB').show()
    x = np.expand_dims(x, axis=0)

    softMaxPred = model.predict(x)
    print("prediction from CNN: " + str(softMaxPred) + "\n")
    probs = softmaxToProbs(softMaxPred)

    # plot_model(model, to_file='Model.png')
    # SVG(model_to_dot(model).create(prog='dot', format='svg'))
    maxprob = 0
    maxI = 0
    for j in range(len(probs)):
        # print(str(j) + " : " + str(round(probs[j], 4)))
        if probs[j] > maxprob:
            maxprob = probs[j]
            maxI = j
    # print(softMaxPred)
    print("prediction index: " + str(maxI))
    return maxI, probs


def softmaxToProbs(soft):
    z_exp = [np.math.exp(i) for i in soft[0]]
    sum_z_exp = sum(z_exp)
    return [(i / sum_z_exp) * 100 for i in z_exp]


def predictImage(img_path='my_image.jpg', arrayImg=None, printData=True):
    crops = []
    if arrayImg == None:
        img = image.load_img(img_path)
        crops = np.array(getCropImgs(img, needRotations=False), np.float16)
        crops = np.divide(crops, 255.)
    Image.fromarray(np.array(crops[0]), "RGB").show()

    classes = []
    classes.append("Benign")
    classes.append("InSitu")
    classes.append("Invasive")
    classes.append("Normal")

    compProbs = []
    compProbs.append(0)
    compProbs.append(0)
    compProbs.append(0)
    compProbs.append(0)

    for i in range(len(crops)):
        if printData:
            print("\n\nCrop " + str(i + 1) + " prediction:\n")

        ___, probs = predict(crops[i], modelSavePath, showImg=False)

        for j in range(len(classes)):
            if printData:
                print(str(classes[j]) + " : " + str(round(probs[j], 4)) + "%")
            compProbs[j] += probs[j]

    if printData:
        print("\n\nAverage from all crops\n")

    for j in range(len(classes)):
        if printData:
            print(str(classes[j]) + " : " + str(round(compProbs[j] / 12, 4)) + "%")


#######################################################################

if __name__ == '__main__':
    image_path = "/home/zzhang/Advanced_ML/cancer_detection_homework/collision_prediction/collision_1/"
    feats_path = "/home/zzhang/Advanced_ML/cancer_detection_homework/collision_prediction/model_data_segment_full_20191109.csv"

    x_img_train, x_img_test, y_train, y_test, x_fea_train, x_fea_test = load_dataset(image_path,feats_path)

    model = train(BATCH_SIZE, EPOCHS,x_img_train, x_img_test, y_train, y_test, x_fea_train, x_fea_test)
